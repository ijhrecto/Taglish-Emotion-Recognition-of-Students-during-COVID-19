{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODOLOGY - EMOTION RECOGNITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf 1.15 for capsnet\n",
    "# cudnn 7.4, cuda = 10.0\n",
    "# matplotlin, seaborn\n",
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/project_data\\\\data_test_oversampled.csv', '../data/project_data\\\\data_train_oversampled.csv', '../data/project_data\\\\data_val_oversampled.csv']\n"
     ]
    }
   ],
   "source": [
    "files = glob(\"../data/project_data/data_*_oversampled.csv\")\n",
    "print(files)\n",
    "test_df = pd.read_csv(files[0])\n",
    "train_df = pd.read_csv(files[1])\n",
    "val_df = pd.read_csv(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_train_x, r_train_y = train_df['text'], train_df['sentiment']\n",
    "r_val_x, r_val_y = val_df['text'], val_df['sentiment']\n",
    "r_test_x, r_test_y = test_df['text'], test_df['sentiment']\n",
    "len(r_train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see distribution\n",
    "train_text_len = [len(i.split()) for i in r_train_x]\n",
    "plt.hist(train_text_len)\n",
    "plt.show()\n",
    "# hellow word = len 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 65 max_features 37187\n"
     ]
    }
   ],
   "source": [
    "# count words\n",
    "def count_words(sentences):\n",
    "    counts = {}\n",
    "    for s in sentences:\n",
    "        for word in s.split():\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "    return counts\n",
    "\n",
    "max_len = 65 # based from the word distribution above\n",
    "max_features = len(count_words(r_train_x))\n",
    "print(\"max_len\",max_len,\"max_features\", max_features)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(r_train_x)\n",
    "\n",
    "# convert words to numbers\n",
    "train_x = tokenizer.texts_to_sequences(r_train_x)\n",
    "val_x = tokenizer.texts_to_sequences(r_val_x)\n",
    "test_x = tokenizer.texts_to_sequences(r_test_x)\n",
    "\n",
    "# pad sentences\n",
    "# max_len = min(max_len, len(max(train_x, key=len)))\n",
    "# [hellow world]  \n",
    "# [1 2] max le = 5\n",
    "# [1 2 0 0 0]\n",
    "# [1 2 3 4 5]\n",
    "train_x = sequence.pad_sequences(train_x, padding=\"pre\",maxlen=max_len, truncating=\"post\")\n",
    "val_x = sequence.pad_sequences(val_x, padding=\"pre\",maxlen=max_len, truncating=\"post\")\n",
    "test_x = sequence.pad_sequences(test_x, padding=\"pre\",maxlen=max_len, truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label binarizer : fear = 1, happy =2, ...\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(r_train_y)\n",
    "\n",
    "# use inverse_transform to get the original class names\n",
    "train_y = lb.transform(r_train_y)\n",
    "val_y = lb.transform(r_val_y)\n",
    "test_y = lb.transform(r_test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 'Fear' :: encoding [0 1 0 0 0]\n",
      "label 'Fear' :: encoding [0 1 0 0 0]\n",
      "label 'Angry' :: encoding [1 0 0 0 0]\n",
      "label 'Surprise' :: encoding [0 0 0 0 1]\n",
      "label 'Fear' :: encoding [0 1 0 0 0]\n",
      "label 'Sad' :: encoding [0 0 0 1 0]\n",
      "label 'Surprise' :: encoding [0 0 0 0 1]\n",
      "label 'Sad' :: encoding [0 0 0 1 0]\n",
      "label 'Happy' :: encoding [0 0 1 0 0]\n",
      "label 'Fear' :: encoding [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"label '{r_test_y[i]}' :: encoding {test_y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# emb_file = \"../data/glove.6B.100d.txt\"\n",
    "# emb_size = 100 # glove 100d\n",
    "# emb_file = \"../data/glove.twitter.27B.50d.txt\"\n",
    "# emb_size = 50\n",
    "\n",
    "\n",
    "# get the word embeddings\n",
    "emb_file = \"../data/glove.twitter.27B.100d.txt\" # <- \n",
    "# emb_file = \"../data/glove.twitter.27B.50d.txt\" # <- 80-82% test acc\n",
    "# emb_file = \"../data/glove.twitter.27B.25d.txt\" # <- 76 %\n",
    "emb_size = 100 #50 #100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emb(emb_file):\n",
    "    # extract the embedding data\n",
    "    embeddings_index = {}\n",
    "    f = open(emb_file, encoding=\"utf8\") \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "        \n",
    "    print(f'Found {len(embeddings_index)} word vectors.')\n",
    "    return embeddings_index\n",
    "    \n",
    "def get_embedding_matrix(emb_file, emb_size):\n",
    "    embeddings_index = extract_emb(emb_file)\n",
    "\n",
    "    # get the embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = max_features #min(max_features, len(word_index))\n",
    "\n",
    "    # do we nee to add plus 1 in nb_words?\n",
    "    embedding_matrix = np.zeros((nb_words, emb_size)) \n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue # disregard first\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# hyper parameter tuning\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study how attenion layer works\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'step_dim': self.step_dim,\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias':self.bias\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# F1\n",
    "class F1Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            y_pred = (y_pred > 0.35).astype(int)\n",
    "            score = f1_score(self.y_val, y_pred, average=\"micro\")\n",
    "            print(\"\\n F1 Score - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
    "\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_ylabel('True label')\n",
    "    axes.set_xlabel('Predicted label')\n",
    "    axes.set_title(\"Confusion Matrix for the class - \" + class_label)\n",
    "\n",
    "def get_predict_metrics(pred, test_y):\n",
    "    # classification report\n",
    "    pred_y = np.argmax(pred, axis=1)\n",
    "    true_y = np.argmax(test_y, axis=1)\n",
    "    print(classification_report(true_y, pred_y))\n",
    "\n",
    "    # visualize confusion matrix\n",
    "    \n",
    "    mlt_cm = multilabel_confusion_matrix(true_y, pred_y)\n",
    "    labels = [\"\".join(\"c\" + str(i)) for i in range(0, 6)]\n",
    "    fig, ax = plt.subplots(5, 1, figsize=(12, 20))\n",
    "    for axes, cfs_matrix, label in zip(ax.flatten(), mlt_cm, labels):\n",
    "        print_confusion_matrix(cfs_matrix, axes, label, [\"N\", \"Y\"])\n",
    "    fig.tight_layout()\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "# model plot\n",
    "def make_plot(loss, val_loss, acc, val_acc):\n",
    "    t = np.arange(1,len(loss)+1,1)\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,3))\n",
    "    plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "    ax1.plot(t, loss)\n",
    "    ax1.plot(t, val_loss)\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('Train vs Val loss')\n",
    "    ax1.legend(['train','val'], ncol=2, loc='upper right')\n",
    "\n",
    "    ax2.plot(t, acc)\n",
    "    ax2.plot(t, val_acc)\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('acc')\n",
    "    ax2.set_title('Train vs Val acc')\n",
    "    ax2.legend(['train','val'], ncol=2, loc='upper right')\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def BiGRUAttenGPU(input_dropout = 0.25, inter_dropout = 0.25,spatial_dropout = True, gru_units=[64], dense_units=[32], \n",
    "                  has_flatten=False, mask_zero = False, has_capsule=[]):\n",
    "    \n",
    "    # input layers\n",
    "    inp = Input(shape=(max_len, ))\n",
    "    \n",
    "    # embedding layers\n",
    "    x = Embedding(max_features,emb_size,weights=[embedding_matrix],trainable=False, mask_zero=mask_zero)(inp)# mask_zero is to ignore the zero paddings\n",
    "    \n",
    "    # dropout layer\n",
    "    if input_dropout != 1: # if 1 = no dropout\n",
    "        if spatial_dropout:\n",
    "            x = SpatialDropout1D(input_dropout)(x)\n",
    "\n",
    "        else:\n",
    "            x = Dropout(input_dropout)(x)\n",
    "            \n",
    "    # BiGRU layer\n",
    "    for gru_unit in gru_units:\n",
    "        x = Bidirectional(GRU(gru_unit, return_sequences=True, activation='tanh', recurrent_activation = 'sigmoid', recurrent_dropout=0, unroll=False, use_bias=True, reset_after =True))(x) # setup for GPU GRU\n",
    "    \n",
    "    # attention layer\n",
    "    x = Attention(max_len)(x)\n",
    "    x = Dropout(inter_dropout)(x)\n",
    "    \n",
    "    # capsule layer\n",
    "    if len(has_capsule) != 0: # capsule should be list containing the three param below\n",
    "        x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "    # conv layer\n",
    "#     for cnn_unit in cnn_units:\n",
    "#         x = Conv1D()\n",
    "    \n",
    "    # dense layer\n",
    "    for dense_unit in dense_units:\n",
    "        x = Dense(dense_unit, activation=\"relu\")(x) # 256\n",
    "        if inter_dropout != 1: # if 1 = no dropout\n",
    "            x = Dropout(inter_dropout)(x)\n",
    "    \n",
    "    # flatten layer\n",
    "    if has_flatten:\n",
    "        x = Flatten()(x)\n",
    "    \n",
    "    # classification layer\n",
    "    x = Dense(5, activation=\"softmax\")(x)\n",
    "    \n",
    "    # build model\n",
    "    model = Model(inputs=inp, outputs=x, name='BiGRUAttenGPU')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without dropout = 80% for 100d word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm,trange\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - Brute Force Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "emb_size = 100\n",
    "emb_file = \"../data/glove.twitter.27B.100d.txt\"\n",
    "embedding_matrix = get_embedding_matrix(emb_file, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainig_start\n",
      "================================================================\n",
      "================================================================\n",
      "MODEL START\n",
      "emb_size : batch_size : gu : du : sd : inpd : intd\n",
      "100 : 256 : [24] : [12] : False : 0.2 : 0.2\n",
      "Model: \"BiGRUAttenGPU\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 65)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 65, 100)           3718700   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 65, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 65, 48)            18144     \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 48)                113       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                588       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 65        \n",
      "=================================================================\n",
      "Total params: 3,737,610\n",
      "Trainable params: 18,910\n",
      "Non-trainable params: 3,718,700\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "131/131 [==============================] - 3s 20ms/step - loss: 1.5784 - acc: 0.2663 - val_loss: 1.5084 - val_acc: 0.3530\n",
      "Epoch 2/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.4783 - acc: 0.3493 - val_loss: 1.4224 - val_acc: 0.4082\n",
      "Epoch 3/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.4092 - acc: 0.4118 - val_loss: 1.3517 - val_acc: 0.4440\n",
      "Epoch 4/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 1.3557 - acc: 0.4436 - val_loss: 1.2925 - val_acc: 0.4660\n",
      "Epoch 5/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 1.3079 - acc: 0.4735 - val_loss: 1.2076 - val_acc: 0.5164\n",
      "Epoch 6/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 1.2724 - acc: 0.4984 - val_loss: 1.1606 - val_acc: 0.5462\n",
      "Epoch 7/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.2424 - acc: 0.5095 - val_loss: 1.1347 - val_acc: 0.5613\n",
      "Epoch 8/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.2133 - acc: 0.5321 - val_loss: 1.0842 - val_acc: 0.5768\n",
      "Epoch 9/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.1920 - acc: 0.5399 - val_loss: 1.0480 - val_acc: 0.5918\n",
      "Epoch 10/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.1740 - acc: 0.5515 - val_loss: 1.0539 - val_acc: 0.5909\n",
      "Epoch 11/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.1576 - acc: 0.5569 - val_loss: 1.0308 - val_acc: 0.6136\n",
      "Epoch 12/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.1343 - acc: 0.5685 - val_loss: 0.9864 - val_acc: 0.6293\n",
      "Epoch 13/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.1183 - acc: 0.5764 - val_loss: 0.9952 - val_acc: 0.6238\n",
      "Epoch 14/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.1105 - acc: 0.5786 - val_loss: 0.9584 - val_acc: 0.6360\n",
      "Epoch 15/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0958 - acc: 0.5888 - val_loss: 0.9550 - val_acc: 0.6386\n",
      "Epoch 16/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0857 - acc: 0.5911 - val_loss: 0.9633 - val_acc: 0.6377\n",
      "Epoch 17/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0721 - acc: 0.5966 - val_loss: 0.9228 - val_acc: 0.6506\n",
      "Epoch 18/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0601 - acc: 0.6040 - val_loss: 0.9222 - val_acc: 0.6539\n",
      "Epoch 19/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0642 - acc: 0.6038 - val_loss: 0.9254 - val_acc: 0.6487\n",
      "Epoch 20/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0466 - acc: 0.6089 - val_loss: 0.9141 - val_acc: 0.6566\n",
      "Epoch 21/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 1.0415 - acc: 0.6136 - val_loss: 0.9093 - val_acc: 0.6618\n",
      "Epoch 22/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0435 - acc: 0.6107 - val_loss: 0.9173 - val_acc: 0.6592\n",
      "Epoch 23/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0332 - acc: 0.6167 - val_loss: 0.9111 - val_acc: 0.6546\n",
      "Epoch 24/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 1.0242 - acc: 0.6193 - val_loss: 0.9068 - val_acc: 0.6640\n",
      "Epoch 25/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 1.0186 - acc: 0.6206 - val_loss: 0.8939 - val_acc: 0.6718\n",
      "Epoch 26/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 1.0108 - acc: 0.6244 - val_loss: 0.8818 - val_acc: 0.6745\n",
      "Epoch 27/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 1.0014 - acc: 0.6277 - val_loss: 0.8738 - val_acc: 0.6749\n",
      "Epoch 28/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 1.0035 - acc: 0.6285 - val_loss: 0.8865 - val_acc: 0.6709\n",
      "Epoch 29/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9984 - acc: 0.6305 - val_loss: 0.8780 - val_acc: 0.6702\n",
      "Epoch 30/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9948 - acc: 0.6327 - val_loss: 0.9251 - val_acc: 0.6587\n",
      "Epoch 31/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9861 - acc: 0.6372 - val_loss: 0.8731 - val_acc: 0.6759\n",
      "Epoch 32/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9823 - acc: 0.6381 - val_loss: 0.8533 - val_acc: 0.6881\n",
      "Epoch 33/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9801 - acc: 0.6383 - val_loss: 0.8579 - val_acc: 0.6840\n",
      "Epoch 34/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9759 - acc: 0.6398 - val_loss: 0.8584 - val_acc: 0.6835\n",
      "Epoch 35/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9709 - acc: 0.6432 - val_loss: 0.8646 - val_acc: 0.6790\n",
      "Epoch 36/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9644 - acc: 0.6447 - val_loss: 0.8466 - val_acc: 0.6869\n",
      "Epoch 37/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9642 - acc: 0.6475 - val_loss: 0.8554 - val_acc: 0.6826\n",
      "Epoch 38/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9553 - acc: 0.6481 - val_loss: 0.8619 - val_acc: 0.6824\n",
      "Epoch 39/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9597 - acc: 0.6468 - val_loss: 0.8733 - val_acc: 0.6721\n",
      "Epoch 40/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.9578 - acc: 0.6485 - val_loss: 0.8770 - val_acc: 0.6797\n",
      "Epoch 41/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.9563 - acc: 0.6484 - val_loss: 0.8503 - val_acc: 0.6859\n",
      "Epoch 42/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9515 - acc: 0.6501 - val_loss: 0.8684 - val_acc: 0.6802\n",
      "Epoch 43/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9528 - acc: 0.6511 - val_loss: 0.8549 - val_acc: 0.6838\n",
      "Epoch 44/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9441 - acc: 0.6542 - val_loss: 0.8370 - val_acc: 0.6890\n",
      "Epoch 45/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9444 - acc: 0.6549 - val_loss: 0.8487 - val_acc: 0.6869\n",
      "Epoch 46/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9383 - acc: 0.6569 - val_loss: 0.8257 - val_acc: 0.6936\n",
      "Epoch 47/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9424 - acc: 0.6571 - val_loss: 0.8380 - val_acc: 0.6890\n",
      "Epoch 48/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9334 - acc: 0.6605 - val_loss: 0.8471 - val_acc: 0.6857\n",
      "Epoch 49/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9368 - acc: 0.6569 - val_loss: 0.8352 - val_acc: 0.6921\n",
      "Epoch 50/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9253 - acc: 0.6603 - val_loss: 0.8372 - val_acc: 0.6967\n",
      "Epoch 51/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.9232 - acc: 0.6633 - val_loss: 0.8359 - val_acc: 0.6888\n",
      "Epoch 52/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.9299 - acc: 0.6603 - val_loss: 0.8426 - val_acc: 0.6900\n",
      "Epoch 53/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.9241 - acc: 0.6627 - val_loss: 0.8359 - val_acc: 0.6933\n",
      "Epoch 54/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9240 - acc: 0.6627 - val_loss: 0.8283 - val_acc: 0.6926\n",
      "Epoch 55/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.9203 - acc: 0.6655 - val_loss: 0.8173 - val_acc: 0.6960\n",
      "Epoch 56/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.9206 - acc: 0.6627 - val_loss: 0.8219 - val_acc: 0.6962\n",
      "Epoch 57/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.9174 - acc: 0.6625 - val_loss: 0.8303 - val_acc: 0.6917\n",
      "Epoch 58/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.9109 - acc: 0.6665 - val_loss: 0.8177 - val_acc: 0.7000\n",
      "Epoch 59/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.9083 - acc: 0.6685 - val_loss: 0.8064 - val_acc: 0.7022\n",
      "Epoch 60/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.9092 - acc: 0.6680 - val_loss: 0.8362 - val_acc: 0.6881\n",
      "Epoch 61/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9116 - acc: 0.6691 - val_loss: 0.8286 - val_acc: 0.6943\n",
      "Epoch 62/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9061 - acc: 0.6700 - val_loss: 0.8128 - val_acc: 0.6986\n",
      "Epoch 63/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9114 - acc: 0.6701 - val_loss: 0.8156 - val_acc: 0.6957\n",
      "Epoch 64/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9124 - acc: 0.6663 - val_loss: 0.8279 - val_acc: 0.6941\n",
      "Epoch 65/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.9014 - acc: 0.6714 - val_loss: 0.8221 - val_acc: 0.6926\n",
      "Epoch 66/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8969 - acc: 0.6723 - val_loss: 0.8178 - val_acc: 0.6969\n",
      "Epoch 67/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9001 - acc: 0.6699 - val_loss: 0.8271 - val_acc: 0.6938\n",
      "Epoch 68/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9021 - acc: 0.6700 - val_loss: 0.8209 - val_acc: 0.6955\n",
      "Epoch 69/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8952 - acc: 0.6756 - val_loss: 0.8078 - val_acc: 0.7034\n",
      "Epoch 70/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8919 - acc: 0.6761 - val_loss: 0.8093 - val_acc: 0.6981\n",
      "Epoch 71/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.9015 - acc: 0.6730 - val_loss: 0.8185 - val_acc: 0.6926\n",
      "Epoch 72/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8928 - acc: 0.6749 - val_loss: 0.7961 - val_acc: 0.7086\n",
      "Epoch 73/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8889 - acc: 0.6768 - val_loss: 0.8096 - val_acc: 0.7019\n",
      "Epoch 74/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8952 - acc: 0.6747 - val_loss: 0.7960 - val_acc: 0.7041\n",
      "Epoch 75/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8878 - acc: 0.6792 - val_loss: 0.8005 - val_acc: 0.7062\n",
      "Epoch 76/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8845 - acc: 0.6753 - val_loss: 0.8085 - val_acc: 0.7024\n",
      "Epoch 77/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8866 - acc: 0.6775 - val_loss: 0.8025 - val_acc: 0.7010\n",
      "Epoch 78/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8867 - acc: 0.6782 - val_loss: 0.8038 - val_acc: 0.7048\n",
      "Epoch 79/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8801 - acc: 0.6796 - val_loss: 0.8180 - val_acc: 0.6895\n",
      "Epoch 80/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8807 - acc: 0.6781 - val_loss: 0.8078 - val_acc: 0.6948\n",
      "Epoch 81/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8837 - acc: 0.6784 - val_loss: 0.8167 - val_acc: 0.6917\n",
      "Epoch 82/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8824 - acc: 0.6795 - val_loss: 0.8027 - val_acc: 0.7000\n",
      "Epoch 83/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8874 - acc: 0.6768 - val_loss: 0.8040 - val_acc: 0.7005\n",
      "Epoch 84/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8781 - acc: 0.6809 - val_loss: 0.8310 - val_acc: 0.6909\n",
      "Epoch 85/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8784 - acc: 0.6829 - val_loss: 0.8027 - val_acc: 0.6979\n",
      "Epoch 86/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8740 - acc: 0.6813 - val_loss: 0.8112 - val_acc: 0.6957\n",
      "Epoch 87/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8772 - acc: 0.6835 - val_loss: 0.8066 - val_acc: 0.6988\n",
      "Epoch 88/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8761 - acc: 0.6784 - val_loss: 0.8044 - val_acc: 0.7041\n",
      "Epoch 89/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8754 - acc: 0.6829 - val_loss: 0.8077 - val_acc: 0.7003\n",
      "Epoch 90/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8740 - acc: 0.6825 - val_loss: 0.8126 - val_acc: 0.6943\n",
      "Epoch 91/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8775 - acc: 0.6792 - val_loss: 0.8037 - val_acc: 0.6972\n",
      "Epoch 92/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8688 - acc: 0.6832 - val_loss: 0.8073 - val_acc: 0.7012\n",
      "Epoch 93/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8694 - acc: 0.6825 - val_loss: 0.8066 - val_acc: 0.6986\n",
      "Epoch 94/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8677 - acc: 0.6822 - val_loss: 0.8115 - val_acc: 0.6986\n",
      "Epoch 95/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8729 - acc: 0.6826 - val_loss: 0.7926 - val_acc: 0.7067\n",
      "Epoch 96/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8639 - acc: 0.6877 - val_loss: 0.8031 - val_acc: 0.6986\n",
      "Epoch 97/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8671 - acc: 0.6841 - val_loss: 0.8005 - val_acc: 0.7005\n",
      "Epoch 98/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8678 - acc: 0.6869 - val_loss: 0.8007 - val_acc: 0.7012\n",
      "Epoch 99/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8583 - acc: 0.6894 - val_loss: 0.7986 - val_acc: 0.7041\n",
      "Epoch 100/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8685 - acc: 0.6832 - val_loss: 0.7830 - val_acc: 0.7058\n",
      "Epoch 101/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8649 - acc: 0.6857 - val_loss: 0.8016 - val_acc: 0.7003\n",
      "Epoch 102/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8636 - acc: 0.6878 - val_loss: 0.8014 - val_acc: 0.7010\n",
      "Epoch 103/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8639 - acc: 0.6842 - val_loss: 0.8034 - val_acc: 0.6986\n",
      "Epoch 104/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8622 - acc: 0.6845 - val_loss: 0.7848 - val_acc: 0.7072\n",
      "Epoch 105/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8646 - acc: 0.6871 - val_loss: 0.7875 - val_acc: 0.7038\n",
      "Epoch 106/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8613 - acc: 0.6880 - val_loss: 0.7900 - val_acc: 0.7055\n",
      "Epoch 107/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8592 - acc: 0.6869 - val_loss: 0.7897 - val_acc: 0.7091\n",
      "Epoch 108/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8568 - acc: 0.6897 - val_loss: 0.8004 - val_acc: 0.7003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8658 - acc: 0.6856 - val_loss: 0.7871 - val_acc: 0.7022\n",
      "Epoch 110/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8553 - acc: 0.6878 - val_loss: 0.7886 - val_acc: 0.7050\n",
      "Epoch 111/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8552 - acc: 0.6904 - val_loss: 0.7986 - val_acc: 0.6986\n",
      "Epoch 112/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8588 - acc: 0.6881 - val_loss: 0.7940 - val_acc: 0.7019\n",
      "Epoch 113/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8603 - acc: 0.6908 - val_loss: 0.8103 - val_acc: 0.6969\n",
      "Epoch 114/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8570 - acc: 0.6917 - val_loss: 0.8010 - val_acc: 0.7029\n",
      "Epoch 115/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8494 - acc: 0.6921 - val_loss: 0.7999 - val_acc: 0.7038\n",
      "Epoch 116/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8494 - acc: 0.6885 - val_loss: 0.7896 - val_acc: 0.7036\n",
      "Epoch 117/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8511 - acc: 0.6904 - val_loss: 0.7893 - val_acc: 0.7110\n",
      "Epoch 118/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8533 - acc: 0.6909 - val_loss: 0.8157 - val_acc: 0.6938\n",
      "Epoch 119/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8464 - acc: 0.6930 - val_loss: 0.7991 - val_acc: 0.6962\n",
      "Epoch 120/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8482 - acc: 0.6917 - val_loss: 0.7782 - val_acc: 0.7101\n",
      "Epoch 121/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8525 - acc: 0.6908 - val_loss: 0.8024 - val_acc: 0.6984\n",
      "Epoch 122/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8510 - acc: 0.6920 - val_loss: 0.7937 - val_acc: 0.7041\n",
      "Epoch 123/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8490 - acc: 0.6907 - val_loss: 0.8043 - val_acc: 0.6960\n",
      "Epoch 124/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8485 - acc: 0.6913 - val_loss: 0.7911 - val_acc: 0.7062\n",
      "Epoch 125/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8494 - acc: 0.6939 - val_loss: 0.7830 - val_acc: 0.7117\n",
      "Epoch 126/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8553 - acc: 0.6907 - val_loss: 0.7895 - val_acc: 0.7065\n",
      "Epoch 127/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8431 - acc: 0.6949 - val_loss: 0.7916 - val_acc: 0.7050\n",
      "Epoch 128/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8423 - acc: 0.6993 - val_loss: 0.7810 - val_acc: 0.7110\n",
      "Epoch 129/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8413 - acc: 0.6967 - val_loss: 0.7759 - val_acc: 0.7139\n",
      "Epoch 130/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8482 - acc: 0.6950 - val_loss: 0.7879 - val_acc: 0.7072\n",
      "Epoch 131/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8437 - acc: 0.6935 - val_loss: 0.7805 - val_acc: 0.7105\n",
      "Epoch 132/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8410 - acc: 0.6944 - val_loss: 0.8025 - val_acc: 0.6972\n",
      "Epoch 133/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8448 - acc: 0.6938 - val_loss: 0.7845 - val_acc: 0.7084\n",
      "Epoch 134/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8493 - acc: 0.6922 - val_loss: 0.7836 - val_acc: 0.7022\n",
      "Epoch 135/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8411 - acc: 0.6945 - val_loss: 0.7915 - val_acc: 0.7024\n",
      "Epoch 136/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8402 - acc: 0.6948 - val_loss: 0.7988 - val_acc: 0.6967\n",
      "Epoch 137/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8422 - acc: 0.6931 - val_loss: 0.7872 - val_acc: 0.7053\n",
      "Epoch 138/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8468 - acc: 0.6956 - val_loss: 0.7833 - val_acc: 0.7122\n",
      "Epoch 139/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8404 - acc: 0.6952 - val_loss: 0.7900 - val_acc: 0.7017\n",
      "Epoch 140/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8412 - acc: 0.6981 - val_loss: 0.7972 - val_acc: 0.7053\n",
      "Epoch 141/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8375 - acc: 0.6949 - val_loss: 0.7825 - val_acc: 0.7060\n",
      "Epoch 142/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8436 - acc: 0.6940 - val_loss: 0.7869 - val_acc: 0.7015\n",
      "Epoch 143/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8421 - acc: 0.6931 - val_loss: 0.7833 - val_acc: 0.7012\n",
      "Epoch 144/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8427 - acc: 0.6959 - val_loss: 0.7981 - val_acc: 0.6967\n",
      "Epoch 145/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8397 - acc: 0.6976 - val_loss: 0.7933 - val_acc: 0.7010\n",
      "Epoch 146/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8396 - acc: 0.6974 - val_loss: 0.7776 - val_acc: 0.7093\n",
      "Epoch 147/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8393 - acc: 0.6963 - val_loss: 0.7989 - val_acc: 0.7003\n",
      "Epoch 148/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8328 - acc: 0.6984 - val_loss: 0.7744 - val_acc: 0.7115\n",
      "Epoch 149/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8342 - acc: 0.6990 - val_loss: 0.7898 - val_acc: 0.7060\n",
      "Epoch 150/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8360 - acc: 0.6984 - val_loss: 0.7932 - val_acc: 0.6998\n",
      "Epoch 151/300\n",
      "131/131 [==============================] - 2s 13ms/step - loss: 0.8350 - acc: 0.6944 - val_loss: 0.7840 - val_acc: 0.7086\n",
      "Epoch 152/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8340 - acc: 0.6977 - val_loss: 0.7892 - val_acc: 0.7036\n",
      "Epoch 153/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8347 - acc: 0.6985 - val_loss: 0.7893 - val_acc: 0.7029\n",
      "Epoch 154/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8370 - acc: 0.6971 - val_loss: 0.7793 - val_acc: 0.7122\n",
      "Epoch 155/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8294 - acc: 0.6996 - val_loss: 0.7963 - val_acc: 0.7024\n",
      "Epoch 156/300\n",
      "131/131 [==============================] - 2s 12ms/step - loss: 0.8275 - acc: 0.6980 - val_loss: 0.7770 - val_acc: 0.7127\n",
      "Epoch 157/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8372 - acc: 0.6981 - val_loss: 0.7913 - val_acc: 0.7043\n",
      "Epoch 158/300\n",
      "131/131 [==============================] - 2s 19ms/step - loss: 0.8405 - acc: 0.6942 - val_loss: 0.7797 - val_acc: 0.7079\n",
      "Epoch 159/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8358 - acc: 0.6981 - val_loss: 0.7763 - val_acc: 0.7089\n",
      "Epoch 160/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8311 - acc: 0.6984 - val_loss: 0.7953 - val_acc: 0.7060\n",
      "Epoch 161/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8309 - acc: 0.6986 - val_loss: 0.7753 - val_acc: 0.7115\n",
      "Epoch 162/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8305 - acc: 0.6954 - val_loss: 0.8102 - val_acc: 0.6955\n",
      "Epoch 163/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8281 - acc: 0.6998 - val_loss: 0.7859 - val_acc: 0.7084\n",
      "Epoch 164/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8328 - acc: 0.7000 - val_loss: 0.7779 - val_acc: 0.7048\n",
      "Epoch 165/300\n",
      "131/131 [==============================] - 3s 21ms/step - loss: 0.8304 - acc: 0.7020 - val_loss: 0.8022 - val_acc: 0.7012oss: 0.8255\n",
      "Epoch 166/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8316 - acc: 0.7005 - val_loss: 0.7914 - val_acc: 0.7043\n",
      "Epoch 167/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8314 - acc: 0.6981 - val_loss: 0.7880 - val_acc: 0.7043\n",
      "Epoch 168/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8250 - acc: 0.6994 - val_loss: 0.7840 - val_acc: 0.7046\n",
      "Epoch 169/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8343 - acc: 0.6957 - val_loss: 0.7922 - val_acc: 0.6988\n",
      "Epoch 170/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8242 - acc: 0.7021 - val_loss: 0.7767 - val_acc: 0.7103\n",
      "Epoch 171/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8240 - acc: 0.7021 - val_loss: 0.7876 - val_acc: 0.7005\n",
      "Epoch 172/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8280 - acc: 0.7001 - val_loss: 0.7725 - val_acc: 0.7079\n",
      "Epoch 173/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8238 - acc: 0.7027 - val_loss: 0.7798 - val_acc: 0.7072\n",
      "Epoch 174/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8314 - acc: 0.6993 - val_loss: 0.7949 - val_acc: 0.7055\n",
      "Epoch 175/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8289 - acc: 0.7024 - val_loss: 0.8025 - val_acc: 0.6988\n",
      "Epoch 176/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8255 - acc: 0.6998 - val_loss: 0.7825 - val_acc: 0.7038\n",
      "Epoch 177/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8231 - acc: 0.7023 - val_loss: 0.7813 - val_acc: 0.7053\n",
      "Epoch 178/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8243 - acc: 0.7015 - val_loss: 0.7883 - val_acc: 0.7041\n",
      "Epoch 179/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8211 - acc: 0.7060 - val_loss: 0.7964 - val_acc: 0.7034\n",
      "Epoch 180/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8307 - acc: 0.7001 - val_loss: 0.8177 - val_acc: 0.6945\n",
      "Epoch 181/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8279 - acc: 0.7014 - val_loss: 0.7833 - val_acc: 0.7055\n",
      "Epoch 182/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8252 - acc: 0.7035 - val_loss: 0.7850 - val_acc: 0.7034\n",
      "Epoch 183/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8258 - acc: 0.7011 - val_loss: 0.7790 - val_acc: 0.7070\n",
      "Epoch 184/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8210 - acc: 0.7035 - val_loss: 0.7917 - val_acc: 0.7005\n",
      "Epoch 185/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8273 - acc: 0.7005 - val_loss: 0.7773 - val_acc: 0.7093\n",
      "Epoch 186/300\n",
      "131/131 [==============================] - 2s 14ms/step - loss: 0.8255 - acc: 0.7000 - val_loss: 0.7824 - val_acc: 0.7105\n",
      "Epoch 187/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8252 - acc: 0.7021 - val_loss: 0.7749 - val_acc: 0.7105\n",
      "Epoch 188/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8256 - acc: 0.7008 - val_loss: 0.7780 - val_acc: 0.7096\n",
      "Epoch 189/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8214 - acc: 0.7044 - val_loss: 0.7765 - val_acc: 0.7086\n",
      "Epoch 190/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8212 - acc: 0.7035 - val_loss: 0.7801 - val_acc: 0.7070\n",
      "Epoch 191/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8256 - acc: 0.7037 - val_loss: 0.7688 - val_acc: 0.7160\n",
      "Epoch 192/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8193 - acc: 0.7041 - val_loss: 0.7819 - val_acc: 0.7091\n",
      "Epoch 193/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8221 - acc: 0.7014 - val_loss: 0.7844 - val_acc: 0.7110\n",
      "Epoch 194/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8227 - acc: 0.7021 - val_loss: 0.7742 - val_acc: 0.7146\n",
      "Epoch 195/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8206 - acc: 0.7036 - val_loss: 0.7680 - val_acc: 0.7132\n",
      "Epoch 196/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8249 - acc: 0.7050 - val_loss: 0.7778 - val_acc: 0.7115\n",
      "Epoch 197/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8267 - acc: 0.6992 - val_loss: 0.7828 - val_acc: 0.7081\n",
      "Epoch 198/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8182 - acc: 0.7051 - val_loss: 0.7922 - val_acc: 0.7038\n",
      "Epoch 199/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8193 - acc: 0.7034 - val_loss: 0.7857 - val_acc: 0.7043\n",
      "Epoch 200/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8148 - acc: 0.7051 - val_loss: 0.7725 - val_acc: 0.7079\n",
      "Epoch 201/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8141 - acc: 0.7053 - val_loss: 0.7841 - val_acc: 0.7067\n",
      "Epoch 202/300\n",
      "131/131 [==============================] - 5s 38ms/step - loss: 0.8204 - acc: 0.7023 - val_loss: 0.7797 - val_acc: 0.7077\n",
      "Epoch 203/300\n",
      "131/131 [==============================] - 4s 34ms/step - loss: 0.8207 - acc: 0.7037 - val_loss: 0.7770 - val_acc: 0.7084\n",
      "Epoch 204/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8114 - acc: 0.7057 - val_loss: 0.7779 - val_acc: 0.7058\n",
      "Epoch 205/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8176 - acc: 0.7027 - val_loss: 0.7842 - val_acc: 0.7055\n",
      "Epoch 206/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8155 - acc: 0.7070 - val_loss: 0.7988 - val_acc: 0.7029\n",
      "Epoch 207/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8154 - acc: 0.7029 - val_loss: 0.7848 - val_acc: 0.7043\n",
      "Epoch 208/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8130 - acc: 0.7049 - val_loss: 0.7796 - val_acc: 0.7072\n",
      "Epoch 209/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8215 - acc: 0.7039 - val_loss: 0.7788 - val_acc: 0.7110\n",
      "Epoch 210/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8148 - acc: 0.7072 - val_loss: 0.7876 - val_acc: 0.7019\n",
      "Epoch 211/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8179 - acc: 0.7012 - val_loss: 0.7680 - val_acc: 0.7175\n",
      "Epoch 212/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8109 - acc: 0.7055 - val_loss: 0.7781 - val_acc: 0.7098\n",
      "Epoch 213/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8197 - acc: 0.7007 - val_loss: 0.7791 - val_acc: 0.7091\n",
      "Epoch 214/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8149 - acc: 0.7065 - val_loss: 0.7860 - val_acc: 0.7050\n",
      "Epoch 215/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8098 - acc: 0.7071 - val_loss: 0.7754 - val_acc: 0.7134\n",
      "Epoch 216/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8119 - acc: 0.7024 - val_loss: 0.7873 - val_acc: 0.7029\n",
      "Epoch 217/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8128 - acc: 0.7036 - val_loss: 0.7851 - val_acc: 0.7065\n",
      "Epoch 218/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8117 - acc: 0.7056 - val_loss: 0.7910 - val_acc: 0.7072\n",
      "Epoch 219/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8115 - acc: 0.7064 - val_loss: 0.7977 - val_acc: 0.7024\n",
      "Epoch 220/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8106 - acc: 0.7044 - val_loss: 0.7785 - val_acc: 0.7096\n",
      "Epoch 221/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8089 - acc: 0.7039 - val_loss: 0.7845 - val_acc: 0.7098\n",
      "Epoch 222/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8134 - acc: 0.7042 - val_loss: 0.7905 - val_acc: 0.7065\n",
      "Epoch 223/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8175 - acc: 0.7035 - val_loss: 0.7797 - val_acc: 0.7110\n",
      "Epoch 224/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8073 - acc: 0.7046 - val_loss: 0.7831 - val_acc: 0.7132\n",
      "Epoch 225/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8097 - acc: 0.7078 - val_loss: 0.7825 - val_acc: 0.7081\n",
      "Epoch 226/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8098 - acc: 0.7049 - val_loss: 0.7640 - val_acc: 0.7134\n",
      "Epoch 227/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8160 - acc: 0.7065 - val_loss: 0.7774 - val_acc: 0.7105\n",
      "Epoch 228/300\n",
      "131/131 [==============================] - 3s 19ms/step - loss: 0.8141 - acc: 0.7044 - val_loss: 0.7802 - val_acc: 0.7062\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8059 - acc: 0.7091 - val_loss: 0.7773 - val_acc: 0.7096\n",
      "Epoch 230/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8083 - acc: 0.7080 - val_loss: 0.7677 - val_acc: 0.7153\n",
      "Epoch 231/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8123 - acc: 0.7053 - val_loss: 0.7776 - val_acc: 0.7081\n",
      "Epoch 232/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8095 - acc: 0.7084 - val_loss: 0.7894 - val_acc: 0.7041\n",
      "Epoch 233/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8160 - acc: 0.7065 - val_loss: 0.7786 - val_acc: 0.7127\n",
      "Epoch 234/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8125 - acc: 0.7067 - val_loss: 0.7636 - val_acc: 0.7191\n",
      "Epoch 235/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8112 - acc: 0.7076 - val_loss: 0.7810 - val_acc: 0.7067\n",
      "Epoch 236/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8099 - acc: 0.7095 - val_loss: 0.7774 - val_acc: 0.7105\n",
      "Epoch 237/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8147 - acc: 0.7049 - val_loss: 0.7817 - val_acc: 0.7050\n",
      "Epoch 238/300\n",
      "131/131 [==============================] - 3s 19ms/step - loss: 0.8067 - acc: 0.7092 - val_loss: 0.7882 - val_acc: 0.7077\n",
      "Epoch 239/300\n",
      "131/131 [==============================] - 3s 22ms/step - loss: 0.8063 - acc: 0.7076 - val_loss: 0.7827 - val_acc: 0.7093\n",
      "Epoch 240/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8036 - acc: 0.7080 - val_loss: 0.7856 - val_acc: 0.7096\n",
      "Epoch 241/300\n",
      "131/131 [==============================] - 3s 19ms/step - loss: 0.8109 - acc: 0.7070 - val_loss: 0.7751 - val_acc: 0.7110loss: 0.807\n",
      "Epoch 242/300\n",
      "131/131 [==============================] - 3s 20ms/step - loss: 0.8111 - acc: 0.7068 - val_loss: 0.7924 - val_acc: 0.7015\n",
      "Epoch 243/300\n",
      "131/131 [==============================] - 3s 21ms/step - loss: 0.8059 - acc: 0.7081 - val_loss: 0.7778 - val_acc: 0.7096\n",
      "Epoch 244/300\n",
      "131/131 [==============================] - 3s 21ms/step - loss: 0.8137 - acc: 0.7062 - val_loss: 0.7829 - val_acc: 0.7120\n",
      "Epoch 245/300\n",
      "131/131 [==============================] - 3s 20ms/step - loss: 0.8077 - acc: 0.7094 - val_loss: 0.7681 - val_acc: 0.7141\n",
      "Epoch 246/300\n",
      "131/131 [==============================] - 3s 20ms/step - loss: 0.8089 - acc: 0.7078 - val_loss: 0.7758 - val_acc: 0.7129\n",
      "Epoch 247/300\n",
      "131/131 [==============================] - 2s 19ms/step - loss: 0.8126 - acc: 0.7063 - val_loss: 0.7660 - val_acc: 0.7155\n",
      "Epoch 248/300\n",
      "131/131 [==============================] - 3s 21ms/step - loss: 0.8105 - acc: 0.7075 - val_loss: 0.7764 - val_acc: 0.7172\n",
      "Epoch 249/300\n",
      "131/131 [==============================] - 3s 26ms/step - loss: 0.8044 - acc: 0.7087 - val_loss: 0.7888 - val_acc: 0.7096\n",
      "Epoch 250/300\n",
      "131/131 [==============================] - 3s 26ms/step - loss: 0.8080 - acc: 0.7089 - val_loss: 0.7685 - val_acc: 0.7141\n",
      "Epoch 251/300\n",
      "131/131 [==============================] - 3s 21ms/step - loss: 0.8144 - acc: 0.7055 - val_loss: 0.7779 - val_acc: 0.7074\n",
      "Epoch 252/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8120 - acc: 0.7068 - val_loss: 0.7728 - val_acc: 0.7093\n",
      "Epoch 253/300\n",
      "131/131 [==============================] - 3s 19ms/step - loss: 0.8044 - acc: 0.7071 - val_loss: 0.7734 - val_acc: 0.7144\n",
      "Epoch 254/300\n",
      "131/131 [==============================] - 3s 20ms/step - loss: 0.8091 - acc: 0.7074 - val_loss: 0.7724 - val_acc: 0.7081\n",
      "Epoch 255/300\n",
      "131/131 [==============================] - 3s 21ms/step - loss: 0.8004 - acc: 0.7079 - val_loss: 0.7929 - val_acc: 0.7017\n",
      "Epoch 256/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8103 - acc: 0.7074 - val_loss: 0.7835 - val_acc: 0.7098\n",
      "Epoch 257/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8038 - acc: 0.7095 - val_loss: 0.7735 - val_acc: 0.7127\n",
      "Epoch 258/300\n",
      "131/131 [==============================] - 3s 22ms/step - loss: 0.8035 - acc: 0.7100 - val_loss: 0.7791 - val_acc: 0.7096loss: 0.8033 - a\n",
      "Epoch 259/300\n",
      "131/131 [==============================] - 3s 22ms/step - loss: 0.8025 - acc: 0.7100 - val_loss: 0.7857 - val_acc: 0.7062\n",
      "Epoch 260/300\n",
      "131/131 [==============================] - 2s 19ms/step - loss: 0.8048 - acc: 0.7099 - val_loss: 0.7868 - val_acc: 0.7098\n",
      "Epoch 261/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8051 - acc: 0.7091 - val_loss: 0.7946 - val_acc: 0.7098\n",
      "Epoch 262/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8055 - acc: 0.7102 - val_loss: 0.7868 - val_acc: 0.7072\n",
      "Epoch 263/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8053 - acc: 0.7119 - val_loss: 0.7796 - val_acc: 0.71050.8042 - acc: 0.7 - ETA: 0s - loss: 0.8046 - acc\n",
      "Epoch 264/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8033 - acc: 0.7084 - val_loss: 0.7928 - val_acc: 0.7053\n",
      "Epoch 265/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8069 - acc: 0.7077 - val_loss: 0.7777 - val_acc: 0.7055\n",
      "Epoch 266/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8047 - acc: 0.7100 - val_loss: 0.7898 - val_acc: 0.7067\n",
      "Epoch 267/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8058 - acc: 0.7086 - val_loss: 0.8033 - val_acc: 0.6995\n",
      "Epoch 268/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8020 - acc: 0.7100 - val_loss: 0.7767 - val_acc: 0.7115\n",
      "Epoch 269/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.7987 - acc: 0.7096 - val_loss: 0.7800 - val_acc: 0.7110\n",
      "Epoch 270/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8020 - acc: 0.7117 - val_loss: 0.7832 - val_acc: 0.7089\n",
      "Epoch 271/300\n",
      "131/131 [==============================] - 2s 19ms/step - loss: 0.8071 - acc: 0.7097 - val_loss: 0.7705 - val_acc: 0.7136\n",
      "Epoch 272/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.7952 - acc: 0.7119 - val_loss: 0.7891 - val_acc: 0.7062\n",
      "Epoch 273/300\n",
      "131/131 [==============================] - 2s 19ms/step - loss: 0.8016 - acc: 0.7091 - val_loss: 0.7837 - val_acc: 0.7091\n",
      "Epoch 274/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.7994 - acc: 0.7081 - val_loss: 0.7928 - val_acc: 0.7046\n",
      "Epoch 275/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8014 - acc: 0.7104 - val_loss: 0.7812 - val_acc: 0.7093\n",
      "Epoch 276/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8001 - acc: 0.7129 - val_loss: 0.7788 - val_acc: 0.7096\n",
      "Epoch 277/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8063 - acc: 0.7092 - val_loss: 0.7922 - val_acc: 0.7046\n",
      "Epoch 278/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.7996 - acc: 0.7117 - val_loss: 0.7898 - val_acc: 0.7038\n",
      "Epoch 279/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8082 - acc: 0.7072 - val_loss: 0.7890 - val_acc: 0.7072\n",
      "Epoch 280/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8004 - acc: 0.7083 - val_loss: 0.7855 - val_acc: 0.7096\n",
      "Epoch 281/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.7976 - acc: 0.7102 - val_loss: 0.7691 - val_acc: 0.7132\n",
      "Epoch 282/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8050 - acc: 0.7092 - val_loss: 0.7754 - val_acc: 0.7098\n",
      "Epoch 283/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.7967 - acc: 0.7103 - val_loss: 0.7872 - val_acc: 0.7036\n",
      "Epoch 284/300\n",
      "131/131 [==============================] - 2s 19ms/step - loss: 0.7993 - acc: 0.7101 - val_loss: 0.7717 - val_acc: 0.7155\n",
      "Epoch 285/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8069 - acc: 0.7101 - val_loss: 0.7822 - val_acc: 0.7091\n",
      "Epoch 286/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.7971 - acc: 0.7136 - val_loss: 0.7773 - val_acc: 0.7110\n",
      "Epoch 287/300\n",
      "131/131 [==============================] - 2s 18ms/step - loss: 0.8006 - acc: 0.7101 - val_loss: 0.7928 - val_acc: 0.7022\n",
      "Epoch 288/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8013 - acc: 0.7118 - val_loss: 0.7778 - val_acc: 0.7124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.7985 - acc: 0.7148 - val_loss: 0.7881 - val_acc: 0.7048\n",
      "Epoch 290/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8035 - acc: 0.7102 - val_loss: 0.7713 - val_acc: 0.7146\n",
      "Epoch 291/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.8001 - acc: 0.7105 - val_loss: 0.7764 - val_acc: 0.7084\n",
      "Epoch 292/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8072 - acc: 0.7079 - val_loss: 0.7743 - val_acc: 0.7148\n",
      "Epoch 293/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8049 - acc: 0.7102 - val_loss: 0.7810 - val_acc: 0.7129\n",
      "Epoch 294/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.7965 - acc: 0.7120 - val_loss: 0.7983 - val_acc: 0.7024\n",
      "Epoch 295/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8002 - acc: 0.7113 - val_loss: 0.7757 - val_acc: 0.7141\n",
      "Epoch 296/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8010 - acc: 0.7104 - val_loss: 0.7900 - val_acc: 0.7096\n",
      "Epoch 297/300\n",
      "131/131 [==============================] - 2s 17ms/step - loss: 0.8030 - acc: 0.7087 - val_loss: 0.7712 - val_acc: 0.7124\n",
      "Epoch 298/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.7911 - acc: 0.7120 - val_loss: 0.7892 - val_acc: 0.7031\n",
      "Epoch 299/300\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.8043 - acc: 0.7105 - val_loss: 0.7888 - val_acc: 0.7027\n",
      "Epoch 300/300\n",
      "131/131 [==============================] - 2s 16ms/step - loss: 0.7957 - acc: 0.7111 - val_loss: 0.7850 - val_acc: 0.7112\n",
      "EVALUATION\n",
      "Train: 0.816, Validation: 0.711, Test: 0.710\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PREDICTION\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65       386\n",
      "           1       0.74      0.63      0.68      1050\n",
      "           2       0.63      0.79      0.70       563\n",
      "           3       0.73      0.70      0.72      1072\n",
      "           4       0.78      0.74      0.76      1116\n",
      "\n",
      "    accuracy                           0.71      4187\n",
      "   macro avg       0.69      0.72      0.70      4187\n",
      "weighted avg       0.72      0.71      0.71      4187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\"\"\"\n",
    "at 64 gru unit its already overfitting\n",
    "\n",
    "32 gru + 16 dense,== 32 gru + 32 dense => 72% with good fit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#  hyperparameters\n",
    "# gru_units = [[16], [32], [64]]# [16,16], [32,32]]\n",
    "# dense_units = [[8], [16], [32]]# [8,8], [16,16]]\n",
    "# spatial_dropout = [False] #[True, False]\n",
    "# input_dropout = [0.2]#[0.1, 0.15, 0.2, 0.25, 0.3]#, 0.25, 0.3, 0.4, 0.5, 1] # 1 = no dropout # 0.2 has better performance\n",
    "# inter_dropout = [0.2]#[0.2, 0.3, 0.5, 0.7, 0.9, 1]#, 0.25, 0.3, 0.4, 0.5, 1] # 1 = no dropout # 0.2 has better performance\n",
    "\n",
    "# gru_units = [[16,16], [32,32]]\n",
    "# dense_units = [[16], [32]]# [8,8], [16,16]]\n",
    "# spatial_dropout = [False] #[True, False]\n",
    "# input_dropout = [0.2]#[0.1, 0.15, 0.2, 0.25, 0.3]#, 0.25, 0.3, 0.4, 0.5, 1] # 1 = no dropout # 0.2 has better performance\n",
    "# inter_dropout = [0.2]#[0.2, 0.3, 0.5, 0.7, 0.9, 1]#, 0.25, 0.3, 0.4, 0.5, 1] # 1 = no dropout # 0.2 has better performance\n",
    "\n",
    "# gru_units = [[32]]\n",
    "# dense_units = [[16]]#[[16], [32]]# [8,8], [16,16]]\n",
    "# spatial_dropout = [False] #[True, False]\n",
    "# input_dropout = [0.2]#[0.1, 0.15, 0.2, 0.25, 0.3]#, 0.25, 0.3, 0.4, 0.5, 1] # 1 = no dropout # 0.2 has better performance\n",
    "# inter_dropout = [0.2]#\n",
    "\n",
    "gru_units = [[24]]\n",
    "dense_units = [[12]]#[[16], [32]]# [8,8], [16,16]]\n",
    "spatial_dropout = [False] #[True, False]\n",
    "input_dropout = [0.2]#[0.1, 0.15, 0.2, 0.25, 0.3]#, 0.25, 0.3, 0.4, 0.5, 1] # 1 = no dropout # 0.2 has better performance\n",
    "inter_dropout = [0.2]#\n",
    "\n",
    "# 16 : 16 = 71 slightly over fit\n",
    "# 24 :16 = 71 good fit\n",
    "\n",
    "\n",
    "# save scores\n",
    "model_hist = []\n",
    "eval_score = []\n",
    "loss_score = []\n",
    "emb_model_ls = {}\n",
    "model_param_info = {}\n",
    "# Train on 16729 samples, validate on 2091 samples\n",
    "epochs = 300 # 300 # 1000 # 60 is usually where it starts to overlap\n",
    "\n",
    "# hyper parameter tuning loop, since keras-tuner is having a problem with the dependencies\n",
    "# emb_file = [\"../data/glove.twitter.27B.25d.txt\",\"../data/glove.twitter.27B.50d.txt\"]\n",
    "# emb_size = [25, 50]\n",
    "print(\"trainig_start\")\n",
    "\n",
    "# for emb_size, embedding_matrix in zip(emb_size_ls, embedding_matrix_ls): # [1] = 100D, [0] = 50D\n",
    "for batch_size in [256]: # 256 best performance\n",
    "    for gu in gru_units:\n",
    "        for du in dense_units:\n",
    "            for sd in spatial_dropout:\n",
    "                for inpd in input_dropout:\n",
    "                    for intd in inter_dropout:\n",
    "                        print(\"================================================================\")\n",
    "                        print(\"================================================================\")\n",
    "                        print(\"MODEL START\")\n",
    "                        print(f\"emb_size : batch_size : gu : du : sd : inpd : intd\")\n",
    "                        print(f\"{emb_size} : {batch_size} : {gu} : {du} : {sd} : {inpd} : {intd}\")\n",
    "                        K.clear_session()\n",
    "                        # TRAINING\n",
    "                    #     model = BiGRUAttenGPU(mask_zero = True, spatial_dropout = False, gru_units=64) # 80\n",
    "                    #     model = BiGRUAtten()\n",
    "                    #     model = BiGRUAttenGPU(mask_zero = False, spatial_dropout = False, gru_units=64, has_capsule=True) # 81\n",
    "                    #     model = RathnayakaModel()\n",
    "                    #     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "                        model = BiGRUAttenGPU(input_dropout = inpd, inter_dropout = intd, spatial_dropout = sd, gru_units=gu, dense_units=du, has_flatten=False, mask_zero = False) # 81\n",
    "                        model.summary()\n",
    "                        hist = model.fit(x=train_x, y=train_y, epochs=epochs, batch_size=batch_size, \n",
    "                                         validation_data=(val_x,val_y))#, callbacks=[es,mc])#, callbacks=[es, mc]) # use epoch 1000 when h5py is fixed\n",
    "                        model_hist.append((model, hist)) # save model and history\n",
    "\n",
    "                        print(\"EVALUATION\")\n",
    "                        # this one doesnt have the saved model yet, uses the last \n",
    "                        _, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
    "                        _, val_acc = model.evaluate(val_x, val_y, verbose=0)\n",
    "                        _, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "                        print('Train: %.3f, Validation: %.3f, Test: %.3f' % (train_acc, val_acc, test_acc))\n",
    "                        eval_score.append( (train_acc,val_acc,test_acc) ) # save model eval score\n",
    "                        # FITTING PLOT\n",
    "                        val_loss = hist.history['val_loss'];val_acc = hist.history['val_acc']\n",
    "                        loss = hist.history['loss'];acc = hist.history['acc']\n",
    "                        make_plot(loss, val_loss, acc, val_acc)\n",
    "                        loss_score.append( (loss, val_loss, acc, val_acc) )# save model loss scor\n",
    "                        # PREDICTION\n",
    "                        print(\"TEST PREDICTION\")\n",
    "                        pred = model.predict(test_x, verbose=0)\n",
    "#                             get_predict_metrics(pred, test_y)\n",
    "                        # print(cls_report)\n",
    "                        pred_y = np.argmax(pred, axis=1)\n",
    "                        true_y = np.argmax(test_y, axis=1)\n",
    "                        print(classification_report(true_y, pred_y))\n",
    "\n",
    "                        model_param_info.update({\"param\": (gu,du,sd,inpd,intd), \n",
    "                                                 \"model\": model_hist,\n",
    "                                                 \"eval\": eval_score,\n",
    "                                                 \"loss\": loss_score})\n",
    "                    \n",
    "\n",
    "emb_model_ls.update({f\"{emb_size}-Dim\":model_param_info})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label 'Fear' :: encoding [0 1 0 0 0]\n",
    "label 'Angry' :: encoding [1 0 0 0 0]\n",
    "label 'Surprise' :: encoding [0 0 0 0 1]\n",
    "label 'Sad' :: encoding [0 0 0 1 0]\n",
    "label 'Happy' :: encoding [0 0 1 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-25997c4f719f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memb_model_ls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'param'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# number of distinct model based from hyper parameter tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# accuracy metric reject - get best test acc by taking the mean of the val acc test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# new_test_acc = emb_model_ls[dim]['eval'][j][-1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_train' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# BEST PARAM\n",
    "{'word_emb': '50D',\n",
    " 'parameter': ([16], [16], False, 0.2)}\n",
    " \n",
    "{'word_emb': '25D',\n",
    " 'parameter': ([16], [8], False, 0.3, 0.5)}\n",
    "\"\"\"\n",
    "# emb_model_ls\n",
    "# 1st Dim = word emb\n",
    "# 2nd Dim = model, acc, history\n",
    "# 3rd Dim = training\n",
    "# num_train = len(emb_model_ls['50-Dim'][0])\n",
    "best_NN_model = {}\n",
    "test_acc = 0\n",
    "\n",
    "for dim in emb_model_ls:\n",
    "    for j in range(len(num_train[dim]['param'])): # number of distinct model based from hyper parameter tuning\n",
    "        # accuracy metric reject - get best test acc by taking the mean of the val acc test\n",
    "        # new_test_acc = emb_model_ls[dim]['eval'][j][-1]\n",
    "        # new_test_acc = sum(new_test_acc)/len(new_test_acc) \n",
    "        \n",
    "        new_test_acc = emb_model_ls[dim]['eval'][j][-1] # -1 because last index is test accuracy\n",
    "        \n",
    "        if new_test_acc > test_acc:\n",
    "            test_acc = new_test_acc\n",
    "            best_emb = dim\n",
    "            best_param = emb_model_ls[dim]['param']\n",
    "            best_model = emb_model_ls[dim]['model']\n",
    "            best_acc = emb_model_ls[dim]['eval']\n",
    "            best_hist = emb_model_ls[dim]['loss']\n",
    "    \n",
    "    # save best param per word dimension\n",
    "    best_NN_model.update({f\"best_{dim}\":{\n",
    "                            \"word_emb\": dim,\n",
    "                            \"parameter\": best_param,\n",
    "                            \"model\": best_model,\n",
    "                            \"accuracies\": best_acc,\n",
    "                            \"history\": best_hist }})\n",
    "# save best param overall\n",
    "best_NN_model.update({\"best_overall\":{\n",
    "                        \"word_emb\": dim,\n",
    "                        \"parameter\": best_param,\n",
    "                        \"model\": best_model,\n",
    "                        \"accuracies\": best_acc,\n",
    "                        \"history\": best_hist }})\n",
    "\n",
    "best_NN_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_parameter(best_model):\n",
    "    \n",
    "    b_gu = best_model['parameter'][0]\n",
    "    b_du = best_model['parameter'][1]\n",
    "    b_sd = best_model['parameter'][2]\n",
    "    b_inp = best_model['parameter'][3]\n",
    "    b_itr = best_model['parameter'][4]\n",
    "    print(\"=====================\")\n",
    "    print(\"      PARAMETERS      \")\n",
    "    print(\"=====================\")\n",
    "    print(f\"Word Embedding Dim : {best_model['word_emb']}\")\n",
    "    print(f\"GRU Units          : {b_gu}\")\n",
    "    print(f\"Dense Units        : {b_du}\") \n",
    "    print(f\"Spatial Dropout    : {b_sd}\")\n",
    "    print(f\"Dropout Val (Input): {b_inp}\")\n",
    "    print(f\"Dropout Val (Inter): {b_itr}\")\n",
    "    print(\"=====================\")\n",
    "    print(\"_________________________________________________________________\")\n",
    "\n",
    "    print(\"=====================\")\n",
    "    print(\"        MODEL\")\n",
    "    print(\"=====================\")\n",
    "    b_model = best_model['model'][0]\n",
    "    b_model.summary()\n",
    "    b_loss = best_model['history'][0]\n",
    "    b_val_loss = best_NN_model['history'][1]\n",
    "    b_acc = best_model['history'][2]\n",
    "    b_val_acc = best_model['history'][3]\n",
    "    make_plot(b_loss, b_val_loss, b_acc, b_val_acc)\n",
    "    print(\"_________________________________________________________________\")\n",
    "\n",
    "    print(\"=====================\")\n",
    "    print(\"     EVALUATION\")\n",
    "    print(\"=====================\")\n",
    "    train_acc = best_model['accuracies'][0]\n",
    "    val_acc = best_model['accuracies'][1]\n",
    "    test_acc = best_model['accuracies'][2]\n",
    "    print('Train: %.3f, Validation: %.3f, Test: %.3f' % (train_acc, val_acc, test_acc))\n",
    "\n",
    "    print(\"_________________________________________________________________\")\n",
    "    print(\"=====================\")\n",
    "    print(\"   TEST PREDICTION   \")\n",
    "    print(\"=====================\")\n",
    "    b_pred = b_model.predict(test_x, verbose=0)\n",
    "    get_predict_metrics(b_pred, test_y)\n",
    "    \n",
    "for key in best_NN_model:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(f\"Best Models : {key}\")\n",
    "    get_best_parameter(best_NN_model[key])\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# BEST PARAM\n",
    "{'word_emb': '50D',\n",
    " 'parameter': ([16], [16], False, 0.2)}\n",
    " \n",
    "{'word_emb': '25D',\n",
    " 'parameter': ([16], [8], False, 0.3, 0.5)}\n",
    "\"\"\"\n",
    "# emb_model_ls\n",
    "# 1st Dim = word emb\n",
    "# 2nd Dim = model, acc, history\n",
    "# 3rd Dim = trainings\n",
    "num_train = len(emb_model_ls[0][0])\n",
    "best_NN_model = {}\n",
    "test_acc = 0\n",
    "for i in range(len(emb_size_ls)): # word embedding\n",
    "    if i == 0 : best_emb = \"25D\"\n",
    "    if i == 1 : best_emb = \"50D\"\n",
    "    if i == 2 : best_emb = \"100D\"\n",
    "    \n",
    "    for j in range(num_train):\n",
    "        \n",
    "        # better accuracy metric\n",
    "        nn_acc = emb_model_ls[i][1][j][1] # accuracy rates list\n",
    "        new_test_acc = nn_acc[-1] # -1 because last index is test accuracy\n",
    "        \n",
    "        # accuracy metric reject - get best test acc by taking the mean of the val acc test\n",
    "        # nn_hist_test = emb_model_ls[i][2][j][1][-1]\n",
    "        # new_test_acc = sum(nn_hist_test)/len(nn_hist_test)\n",
    "        \n",
    "        # update best parameter\n",
    "        if new_test_acc > test_acc:\n",
    "            test_acc = new_test_acc\n",
    "            best_param, best_acc = emb_model_ls[i][1][j]\n",
    "            best_model = emb_model_ls[i][0][j][1]\n",
    "            best_hist = emb_model_ls[i][2][j][1]\n",
    "        else:\n",
    "            test_acc = test_acc\n",
    "            best_param = best_param\n",
    "            best_acc = best_acc\n",
    "            best_model = best_model\n",
    "            best_hist = best_hist\n",
    "            \n",
    "best_NN_model.update({\"word_emb\":best_emb,\n",
    "                      \"parameter\":best_param,\n",
    "                      \"accuracies\":best_acc,\n",
    "                      \"model\":best_model,\n",
    "                      \"history\":best_hist})\n",
    "best_NN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b_gu = best_NN_model['parameter'][0]\n",
    "b_du = best_NN_model['parameter'][1]\n",
    "b_sd = best_NN_model['parameter'][2]\n",
    "b_inp = best_NN_model['parameter'][3]\n",
    "b_itr = best_NN_model['parameter'][4]\n",
    "print(\"=====================\")\n",
    "print(\"      PARAMETERS      \")\n",
    "print(\"=====================\")\n",
    "print(f\"Word Embedding Dim : {best_NN_model['word_emb']}\")\n",
    "print(f\"GRU Units          : {b_gu}\")\n",
    "print(f\"Dense Units        : {b_du}\") \n",
    "print(f\"Spatial Dropout    : {b_sd}\")\n",
    "print(f\"Dropout Val (Input): {b_inp}\")\n",
    "print(f\"Dropout Val (Inter): {b_itr}\")\n",
    "print(\"=====================\")\n",
    "print(\"_________________________________________________________________\")\n",
    "\n",
    "print(\"=====================\")\n",
    "print(\"        MODEL\")\n",
    "print(\"=====================\")\n",
    "b_model = best_NN_model['model'][0]\n",
    "b_model.summary()\n",
    "b_loss = best_NN_model['history'][0]\n",
    "b_val_loss = best_NN_model['history'][1]\n",
    "b_acc = best_NN_model['history'][2]\n",
    "b_val_acc = best_NN_model['history'][3]\n",
    "make_plot(b_loss, b_val_loss, b_acc, b_val_acc)\n",
    "print(\"_________________________________________________________________\")\n",
    "\n",
    "print(\"=====================\")\n",
    "print(\"     EVALUATION\")\n",
    "print(\"=====================\")\n",
    "_, train_acc = b_model.evaluate(train_x, train_y, verbose=0)\n",
    "_, val_acc = b_model.evaluate(val_x, val_y, verbose=0)\n",
    "_, test_acc = b_model.evaluate(test_x, test_y, verbose=0)\n",
    "print('Train: %.3f, Validation: %.3f, Test: %.3f' % (train_acc, val_acc, test_acc))\n",
    "\n",
    "print(\"_________________________________________________________________\")\n",
    "print(\"=====================\")\n",
    "print(\"   TEST PREDICTION   \")\n",
    "print(\"=====================\")\n",
    "b_pred = b_model.predict(test_x, verbose=0)\n",
    "get_predict_metrics(b_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on epoch = 50, batch size = 256\n",
    "dtaa = 80:10:10, 12k data\n",
    "test acc\n",
    "25d = 76 %\n",
    "50d = 82 %\n",
    "100d = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for when we apllied checkpoint saving\n",
    "\"\"\"\n",
    "# load the saved model../data/sentiment_NLP/model_train/_weights_.{epoch:02d}-{val_acc:.2f}.hdf5\n",
    "saved_model = tf.keras.models.load_model('../data/sentiment_NLP/model_train/BiGRUAtten/_weights_.41-0.82.hdf5')\n",
    "# saved_model = load_model('../data/sentiment_NLP/model_train/BiGRUAtten/_weights_.79-0.82.hdf5')\n",
    "# evaluate the model\n",
    "_, train_acc = saved_model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "test different word embedding dimensions\n",
    "\n",
    "kohen kappa? for confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters as hp\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiGRUAtten_modelHP(hp):\n",
    "    # hyper parameters\n",
    "    gru_units = hp.Int(\"gru_units\", min_value=64, max_value=256, step=32)\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=64, max_value=256, step=32)\n",
    "    drop_out = hp.Float(\"dropout_val\", min_value=0.25, max_value=0.5, step=0.125)\n",
    "    drop_out_type = hp.Boolean(\"dropout_type\")\n",
    "    dense_layers = hp.Int(\"dense_layers\", 1, 2)\n",
    "    has_flatten = hp.Boolean(\"has_flatten_layer\")\n",
    "    \n",
    "    \n",
    "    # start model\n",
    "    \n",
    "    inp = Input(shape=(max_len, ))\n",
    "\n",
    "    x = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    # set embedding dropout\n",
    "    x = Dropout(drop_out)(x) if drop_out_type == False else SpatialDropout1D(drop_out)(x)\n",
    "    \n",
    "    # setup to activate GPU GRU\n",
    "#     x = Bidirectional(GRU(gru_units, return_sequences=True))\n",
    "    x = Bidirectional(GRU(gru_units, return_sequences=True, activation='tanh', recurrent_activation = 'sigmoid',\n",
    "                         recurrent_dropout=0, unroll=False, use_bias=True, reset_after =True))(x)\n",
    "    x = Attention(max_len)(x)\n",
    "    \n",
    "    # number of dense layers after the GRU + atten layers\n",
    "    for i in range(dense_layers):\n",
    "        x = Dense(dense_units, activation=\"relu\")(x)\n",
    "        x = Dropout(drop_out)(x)\n",
    "\n",
    "    if has_flatten:\n",
    "        x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(3, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x, name='BiGRUAtten')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.normpath(f'C:/{int(time.time())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_grid = dict(mask_zero=mask_zero,spatial_dropout=spatial_dropout,drop_out_val=drop_out_val,\n",
    "#                  gru_units=gru_units,dense_layers=dense_layers, batch_size=[128, 256], epochs=[50])\n",
    "\n",
    "# ps = PredefinedSplit(test_fold=your_test_fold) # when we already have predefined valid/test set\n",
    "# grif = GridSearchCV(estimator = model, param_grid=para_grid, cv=5)\n",
    "\n",
    "# model = BiGRUAtten_modelling()\n",
    "# model.fit(train_x, train_y, batch_size=256, epochs= 50, validation=(val_x, val_y))\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "LOG_DIR = os.path.normpath(f'C:/{int(time.time())}')\n",
    "\n",
    "# tuning setup\n",
    "tuner = RandomSearch(\n",
    "    BiGRUAtten_modelHP,\n",
    "    objective = \"val_accuracy\",\n",
    "    max_trials = 450, # 1,\n",
    "    executions_per_trial = 1,\n",
    "    directory = LOG_DIR\n",
    "#     directory = f\"../data/{int(time.time())}\"\n",
    ")\n",
    "tuner.search(x=train_x,\n",
    "            y=train_y,\n",
    "            epochs=50,\n",
    "            batch_size=256,\n",
    "            validation_data=(val_x, val_y)#(val_x,val_y),\n",
    "            )\n",
    "\n",
    "# save best model\n",
    "with open(f\"../data/model_tuning/model_tuner_test_as_tuner.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tuner, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import best tuner\n",
    "tuner = pickle.load(open(f\"../data/model_tuning/model_tuner_test_as_tuner.pkl\", \"rb\"))\n",
    "\n",
    "print(tuner.get_best_hyperparameters()[0].values)\n",
    "print(tuner.results_summary())\n",
    "print(tuner.get_best_models()[0].summary()) # get the tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tuner.get_best_models()[0]\n",
    "model.summary()\n",
    "print(\"Start Training\")\n",
    "hist = model.fit(x=train_x, y=train_y, epochs=50, batch_size=256, validation_data=(val_x,val_y))#, callbacks=[es,mc])#, callbacks=[es, mc]) # use epoch 1000 when h5py is fixed\n",
    "# hist = model.fit(x=train_x, y=train_y, epochs=50, batch_size=256, validation_data=(test_x,test_y))#, callbacks=[es,mc])#, callbacks=[es, mc]) # use epoch 1000 when h5py is fixed\n",
    "\n",
    "print(\"EVALUATION\")\n",
    "# this one doesnt have the saved model yet, uses the last \n",
    "_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n",
    "_, val_acc = model.evaluate(val_x, val_y, verbose=0)\n",
    "_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "# _, val_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "# _, test_acc = model.evaluate(val_x, val_y, verbose=0)\n",
    "print('Train: %.3f, Validation: %.3f, Test: %.3f' % (train_acc, val_acc, test_acc))\n",
    "\n",
    "# FITTING\n",
    "val_loss = hist.history['val_loss'];val_acc = hist.history['val_accuracy']\n",
    "loss = hist.history['loss'];acc = hist.history['accuracy']\n",
    "make_plot(loss, val_loss, acc, val_acc)\n",
    "\n",
    "# PREDICTION\n",
    "print(\"TEST PREDICTION\")\n",
    "# pred = model.predict(val_x, verbose=0)\n",
    "# get_predict_metrics(pred, val_y)\n",
    "pred = model.predict(test_x, verbose=0)\n",
    "get_predict_metrics(pred, test_y)\n",
    "# print(cls_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
